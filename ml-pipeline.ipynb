{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b0bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make sure you follow all the steps\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import create_component_from_func\n",
    "from typing import NamedTuple\n",
    "\n",
    "def Download_data():\n",
    "    #upload the zip file downloaded from kaggle to GCS bucket and retrieve the credentials before proceeding\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "\n",
    "    bucket_name = \"pneumonia-classify\"\n",
    "    bucket_folder_path = \"archive.zip\"  \n",
    "    local_directory = \"home/jovyan/archive.zip\"  \n",
    "\n",
    "    client = storage.Client.from_service_account_json('key.json') #include the path of ur credentials\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(bucket_folder_path)\n",
    "    \n",
    "    blob.download_to_filename(local_directory)\n",
    "    print(\"pneumonia-dataset downloaded successfully\")\n",
    "\n",
    "def Load_data():\n",
    "    #extract data from archive folder into train, test folders\n",
    "    import zipfile\n",
    "    \n",
    "    zip_path = 'home/jovyan/archive.zip'\n",
    "    extract_path = 'home/jovyan/'\n",
    "\n",
    "    with zipfile.ZipFile(zip_path,'r') as file:\n",
    "        file.extractall(extract_path)\n",
    "\n",
    "    print(\"data loaded successfully\")\n",
    "\n",
    "def Preprocess_data() -> NamedTuple('Outputs',[('preprocessed_path',str)]):\n",
    "    #data preprocessing\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(\"Preprocessing given data:\")\n",
    "    train_path = '/home/jovyan/train/'\n",
    "    image_height = 227\n",
    "    image_width = 227\n",
    "    class_names = ['normal','pneumonia']\n",
    "\n",
    "    train_data = keras.preprocessing.image_dataset_from_directory(\n",
    "        train_path,\n",
    "        batch_size = 10,\n",
    "        image_size = (227,227),\n",
    "        shuffle = True,\n",
    "        class_names = class_names,\n",
    "        seed = 1234,\n",
    "        subset = 'training',\n",
    "        validation_split = 0.15\n",
    "        )\n",
    "\n",
    "    valid_data = keras.preprocessing.image_dataset_from_directory(\n",
    "        train_path,\n",
    "        batch_size = 10,\n",
    "        image_size = (227,227),\n",
    "        shuffle = True,\n",
    "        seed = 1234,\n",
    "        class_names = class_names,\n",
    "        subset = 'validation',\n",
    "        validation_split = 0.15\n",
    "        )\n",
    "    \n",
    "    tf.data.experimental.save(train_data,'/home/jovyan/preprocess/train/')\n",
    "    print(\"Training data is saved successfully!!\")\n",
    "    tf.data.experimental.save(train_data,'/home/jovyan/preprocess/valid/')\n",
    "    print(\"Validation data is saved successfully!!\")\n",
    "    return '/home/jovyan/preprocess/'\n",
    "\n",
    "def Modelling():\n",
    "    #ResNet50\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    from keras import layers\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Flatten\n",
    "\n",
    "    train_data = tf.data.experimental.load('/home/jovyan/preprocess/train/')\n",
    "    valid_data = tf.data.experimental.load(\"/home/jovyan/preprocess/valid/\")\n",
    "\n",
    "    resnet = Sequential()\n",
    "    pretrained_model = keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        input_shape=(227,227,3),\n",
    "        pooling='avg',\n",
    "        classes='2',\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    for i in pretrained_model.layers:\n",
    "        i.trainable = False\n",
    "\n",
    "    resnet.add(pretrained_model)\n",
    "    resnet.add(Flatten())\n",
    "    resnet.add(Dense(512, activation='relu'))\n",
    "    resnet.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    resnet.compile(loss='sparse_categorical_crossentropy',optimizer= keras.optimizers.SGD(lr=0.0009),metrics=[\"accuracy\"])\n",
    "\n",
    "    model = resnet.fit(\n",
    "        train_data,\n",
    "        validation_data= valid_data,\n",
    "        epochs= 10\n",
    "    )\n",
    "    resnet.save('/home/jovyan/model.h5')\n",
    "    print(\"Model trained succesfully\")\n",
    "\n",
    "def Prediction():\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    saved_model = load_model('/home/jovyan/model.h5')\n",
    "    valid_data = tf.data.experimental.load(\"/home/jovyan/preprocess/valid/\")\n",
    "\n",
    "    predictions = saved_model.predict(valid_data)\n",
    "    \n",
    "    true_label = valid_data.class_names\n",
    "    predicted_classes = np.argmax(predictions,axis=1)\n",
    "\n",
    "    print(\"Confusion matrix\")\n",
    "    cm = confusion_matrix(true_label,predicted_classes)\n",
    "    print(cm)\n",
    "    \n",
    "#components creation\n",
    "download = create_component_from_func(Download_data,packages_to_install = 'google-cloud-storage')\n",
    "load = create_component_from_func(Load_data)\n",
    "preprocess = create_component_from_func(Preprocess_data,base_image = 'tensorflow/tensorflow:2.8')\n",
    "model_v = create_component_from_func(Modelling,base_image = 'tensorflow/tensorflow:2.8')\n",
    "prediction = create_component_from_func(Prediction, packages_to_install=['numpy','scikit-learn'])\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name = 'ml-pipeline',\n",
    "    description = 'pneumonia classifier'\n",
    ")\n",
    "\n",
    "def ml_pipeline():\n",
    "    #attach a existing pvc\n",
    "    pvc = dsl.PipelineVolume(pvc='pneumonia-classifier-volume')\n",
    "\n",
    "    #defining pipeline steps\n",
    "    step_1 = download().add_pvolumes(pvc)\n",
    "\n",
    "    step_2 = load().add_pvolumes(pvc)\n",
    "    step_2.after(step_1)\n",
    "\n",
    "    step_3 = preprocess().add_pvolumes(pvc)\n",
    "    step_3.after(step_2)\n",
    "\n",
    "    step_4 = model_v().add_pvolumes(pvc)\n",
    "    step_4.after(step_3)\n",
    "\n",
    "    step_5 = prediction().add_pvolumes(pvc)\n",
    "    step_5.after(step_4)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    client = kfp.Client()\n",
    "    client.create_run_from_pipeline_func(ml_pipeline,arguments=None,experiment_name='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
